## Custom envs
donkey-generated-track-v0:
  # Normalize AE (+ the rest)
  # TODO: try normalizing reward too
  normalize: "{'norm_obs': True, 'norm_reward': False}"
    # - utils.wrappers.ResidualExpertWrapper:
    #     model_path: ./logs/human/donkey-generated-track-v0_2/policy.pt
    #     d3rlpy_model: True
    #     residual_scale: 0.5
    #     expert_scale: 1.0
    #- utils.wrappers.TimeFeatureWrapper:
    #    test_mode: False
    # - stable_baselines3.common.monitor.Monitor:
    #     filename: None
  env_wrapper:
#    - vec_env_wrapper:
#        - utils.wrappers.VecForceResetWrapper
#    - gym.wrappers.time_limit.TimeLimit:
#          max_episode_steps: 10000
#    - utils.wrappers.HistoryWrapper:
#          horizon: 2
    - utils.wrappers.PreProcessingWrapper:
        crop_ratio: 70
        filter_type: "laplacian"
        min_luminosity_value: 195
        max_luminosity_value: 200
        alpha: 0.15
        beta: 0.9
        log_activated: False
        normalize : True
#    - utils.wrappers.ActionSmoothingWrapper:
#        smoothing_coef: 0.5
  callback:
    - utils.callbacks.ParallelTrainCallback:
        gradient_steps: 200

  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  #learning_rate: !!float 3e-4
  buffer_size: 10000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  # train_freq: [1, "episode"]
  train_freq: 200
  # gradient_steps: -1
  gradient_steps: 256
#  learning_starts: 5000
#  use_sde_at_warmup: True
#  use_sde: True
#  sde_sample_freq: 16
#  policy_kwargs: "dict(log_std_init=-3, net_arch=[256, 256], n_critics=2)"
