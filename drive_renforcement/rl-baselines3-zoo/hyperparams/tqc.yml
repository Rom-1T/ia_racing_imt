## Custom envs

donkey-sigma-v0:
#  normalize: "{'norm_obs': False, 'norm_reward': False}"
  env_wrapper:
##    - vec_env_wrapper:
##        - utils.wrappers.VecForceResetWrapper
##    - gym.wrappers.time_limit.TimeLimit:
##          max_episode_steps: 5000
##    - utils.wrappers.HistoryWrapper:
##          horizon: 2
#    - utils.wrappers.CNNPrepro:
#        crop_Y: 40
#        filter_type: "final_filter"
#        log : False
#        epaisseur : 2
    - utils.wrappers.Deadzone:
        deadzone: 0.15
    - aae-train-donkeycar.ae.wrapper.AutoencoderWrapper:
        ae_path: "/home/rom1/Documents/ia_racing_imt/rl-baselines3-zoo/aae-train-donkeycar/auto-encoder-agent/models/ae-raw.pkl"
        filter : "None"
        degrade : False
        log : False
        epaisseur : 2
        waitingTime : 0
        timeScale : 6
##    - utils.wrappers.ActionSmoothingWrapper:
##        smoothing_coef: 0.5
##    - utils.wrappers.PastThrottle:
##        length: 5
##        auto_encoder_size : 32
  callback:
    - utils.callbacks.ParallelTrainCallback:
        gradient_steps: 200

  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  #learning_rate: !!float 3e-4
  buffer_size: 100000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  # train_freq: [1, "episode"]
  train_freq: 200
  # gradient_steps: -1
  gradient_steps: 256
#  learning_starts: 5000
#  use_sde_at_warmup: True
#  use_sde: True
#  sde_sample_freq: 16
#  policy_kwargs: "dict(log_std_init=-3, net_arch=[256, 256], n_critics=2)"


#  n_timesteps: !!float 2e6
#  policy: 'CnnPolicy'
#  learning_rate: !!float 7.3e-4
#  #learning_rate: !!float 3e-4
#  buffer_size: 10000
#  batch_size: 256
#  ent_coef: 'auto'
#  gamma: 0.99
#  tau: 0.02
#  # train_freq: [1, "episode"]
#  train_freq: 200
#  # gradient_steps: -1
#  gradient_steps: 256
##  learning_starts: 5000
##  use_sde_at_warmup: True
##  use_sde: True
##  sde_sample_freq: 16
##  policy_kwargs: "dict(log_std_init=-3, net_arch=[256, 256], n_critics=2)"

donkey-minimonaco-track-v0:
  normalize: "{'norm_obs': False, 'norm_reward': False}"
  env_wrapper:
#    - vec_env_wrapper:
#        - utils.wrappers.VecForceResetWrapper
#    - gym.wrappers.time_limit.TimeLimit:
#          max_episode_steps: 5000
#    - utils.wrappers.HistoryWrapper:
#          horizon: 2
#    - utils.wrappers.PreProcessingWrapper:
#        crop_ratio: 70
#        filter_type: "laplacian"
#        min_luminosity_value: 195
#        max_luminosity_value: 200
#        alpha: 0.15
#        beta: 0.9
#        log_activated: False
#        normalize: False
    - utils.wrappers.Deadzone:
        deadzone: 0.15
    - aae-train-donkeycar.ae.wrapper.AutoencoderWrapper:
        ae_path: "/home/rom1/Documents/ia_racing_imt/rl-baselines3-zoo/aae-train-donkeycar/auto-encoder-agent/ae-32_simu_degrade_reel_400.pkl"
        filter : True
        degradation : True
#    - utils.wrappers.ActionSmoothingWrapper:
#        smoothing_coef: 0.5
    - utils.wrappers.PastThrottle:
        length: 5
        auto_encoder_size : 32
  callback:
    - utils.callbacks.ParallelTrainCallback:
        gradient_steps: 200

  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  #learning_rate: !!float 3e-4
  buffer_size: 100000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  # train_freq: [1, "episode"]
  train_freq: 200
  # gradient_steps: -1
  gradient_steps: 256
#  learning_starts: 5000
#  use_sde_at_warmup: True
#  use_sde: True
#  sde_sample_freq: 16
#  policy_kwargs: "dict(log_std_init=-3, net_arch=[256, 256], n_critics=2)"

donkey-gray-v0:
  normalize: "{'norm_obs': False, 'norm_reward': False}"
  env_wrapper:
#    - vec_env_wrapper:
#        - utils.wrappers.VecForceResetWrapper
#    - gym.wrappers.time_limit.TimeLimit:
#          max_episode_steps: 5000
#    - utils.wrappers.HistoryWrapper:
#          horizon: 2
#    - utils.wrappers.PreProcessingWrapper:
#        crop_ratio: 70
#        filter_type: "laplacian"
#        min_luminosity_value: 195
#        max_luminosity_value: 200
#        alpha: 0.15
#        beta: 0.9
#        log_activated: False
#        normalize: False
    - utils.wrappers.Deadzone:
        deadzone: 0.15
#    - aae-train-donkeycar.ae.wrapper.AutoencoderWrapper:
#        ae_path: "/home/rom1/Documents/ia_racing_imt/rl-baselines3-zoo/aae-train-donkeycar/auto-encoder-agent/ae-32_simu_degrade_reel_400.pkl"
#        filter : True
#        degrade : True
#    - utils.wrappers.ActionSmoothingWrapper:
#        smoothing_coef: 0.5
#    - utils.wrappers.PastThrottle:
#        length: 5
#        auto_encoder_size : 32
  callback:
    - utils.callbacks.ParallelTrainCallback:
        gradient_steps: 200

  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  #learning_rate: !!float 3e-4
  buffer_size: 100000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  # train_freq: [1, "episode"]
  train_freq: 200
  # gradient_steps: -1
  gradient_steps: 256
#  learning_starts: 5000
#  use_sde_at_warmup: True
#  use_sde: True
#  sde_sample_freq: 16
#  policy_kwargs: "dict(log_std_init=-3, net_arch=[256, 256], n_critics=2)"


donkey-generated-track-v0:
  # Normalize AE (+ the rest)
  # TODO: try normalizing reward too
#  normalize: "{'norm_obs': False, 'norm_reward': False}"
    # - utils.wrappers.ResidualExpertWrapper:
    #     model_path: ./logs/human/donkey-generated-track-v0_2/policy.pt
    #     d3rlpy_model: True
    #     residual_scale: 0.5
    #     expert_scale: 1.0
    #- utils.wrappers.TimeFeatureWrapper:
    #    test_mode: False
    # - stable_baselines3.common.monitor.Monitor:
    #     filename: None
  env_wrapper:
#    - vec_env_wrapper:
#        - utils.wrappers.VecForceResetWrapper
#    - gym.wrappers.time_limit.TimeLimit:
#          max_episode_steps: 10000
#    - utils.wrappers.HistoryWrapper:
#          horizon: 2
    - utils.wrappers.PreProcessingWrapper:
        crop_ratio: 70
        filter_type: "laplacian"
        min_luminosity_value: 195
        max_luminosity_value: 200
        alpha: 0.15
        beta: 0.9
        log_activated: False
        normalize: False
#    - utils.wrappers.SteeringSmoothingWrapper:
#        smoothing_coef: 0.2
  callback:
    - utils.callbacks.ParallelTrainCallback:
        gradient_steps: 200
  n_timesteps: !!float 2e6
  policy: 'CnnPolicy'
  learning_rate: !!float 7.3e-4
  #learning_rate: !!float 3e-4
  buffer_size: 10000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  # train_freq: [1, "episode"]
  train_freq: 200
  # gradient_steps: -1
  gradient_steps: 256
#  learning_starts: 5000
#  use_sde_at_warmup: True
#  use_sde: True
#  sde_sample_freq: 16
#  policy_kwargs: "dict(log_std_init=-3, net_arch=[256, 256], n_critics=2)"
