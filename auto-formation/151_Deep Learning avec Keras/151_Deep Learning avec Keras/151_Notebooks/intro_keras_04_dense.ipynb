{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 1,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<img src=\"https://datascientest.fr/train/assets/logo_datascientest.png\" style=\"height:150px\">\n",
    "\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "<center><h1> Introduction au Deep Learning avec Keras </h1></center>\n",
    "<center><h3>Prédiction à l'aide des Dense Neural Networks</h3></center>\n",
    "<hr style=\"border-width:2px;border-color:#75DFC1\">\n",
    "\n",
    "\n",
    "## Contexte et objectif\n",
    "\n",
    ">Le principal objectif de cet exercice est de créer votre premier réseau de neurones artificiels (*Neural Network*) pour la reconnaissance de chiffres écrits à la main avec le module **Keras**. Le module <b>Keras</b> est accompagnée d'une riche documentation que vous pouvez consulter <a href= https://keras.io/#you-have-just-found-keras)>ici</a>.\n",
    ">\n",
    "> Nous allons construire un réseau de neurones artificiels simple avec deux couches de neurones cachées (*hidden layers*). On rappelle que les couches cachées sont toutes les couches qui se trouvent entre la couche d'entrée (*input layer*) et la couche de sortie (*output layer*).\n",
    "\n",
    "## Compétences requises\n",
    "\n",
    "> * Scikit-learn\n",
    "> * Matplotlib\n",
    "> * Pandas pour la Data Science\n",
    "\n",
    "* Exécutez la cellule ci-dessous pour importer les modules nécessaires à l'exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "function": "preliminary",
    "question_id": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # Pour la manipulation de tableaux\n",
    "\n",
    "import pandas as pd # Pour manipuler des DataFrames pandas\n",
    "\n",
    "import matplotlib.pyplot as plt # Pour l'affichage d'images\n",
    "from matplotlib import cm # Pour importer de nouvelles cartes de couleur\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential # Pour construire un réseau de neurones\n",
    "from keras.layers import Dense # Pour instancier une couche dense\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import itertools # Pour créer des iterateurs\n",
    "\n",
    "from sklearn import metrics # Pour évaluer les modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 1,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Exécutez la cellule ci-dessous pour charger les échantillons d'entraînement et de test de la base de données MNIST du premier exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (60000, 784)\n",
      "Shape of y: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Pour importer le datasets mnist de Keras\n",
    "from keras.datasets.mnist import load_data\n",
    "\n",
    "# Chargement des données MNIST\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "# Changement de forme\n",
    "X_train = X_train.reshape([-1, 28*28])\n",
    "X_test = X_test.reshape([-1, 28*28])\n",
    "\n",
    "# Shape of X_train and y_train\n",
    "print('Shape of X:', X_train.shape)\n",
    "print('Shape of y:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 1,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Pour une meilleure performance du modèle, réduire les pixels des données **X_train** et **X_test** afin qu'ils soient compris entre 0 et 1.\n",
    "\n",
    "<div class='alert alert-success'>\n",
    "Pour réduire les pixels il suffit de diviser l'échantillon entier par 255. Ainsi tous les pixels seront compris entre 0 et 1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "function": "submission",
    "question_id": 1
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 2,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Transformer les labels de **y_train** et **y_test** en vecteurs catégoriels binaires (*one hot*) grâce à la fonction `to_categorical` du sous-module **np_utils** de **keras**.\n",
    "\n",
    "\n",
    "* Extraire dans des variables appelées respectivement **num_pixels** et **num_classes** le nombre de colonnes (pixels) de **X_train** et le nombre de colonnes (classes) de **y_test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "function": "submission",
    "question_id": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 3,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> Nous allons construire notre modèle **séquentiellement**, c'est-à-dire que nous allons le construire couche par couche depuis la couche d'entrée jusqu'à la couche de sortie.\n",
    ">\n",
    "> La construction séquentielle d'un modèle avec **Keras** se fait très facilement avec les étapes suivantes:\n",
    ">\n",
    "> * **Étape 1** : Instancier un modèle avec le constructeur `Sequential` que nous avons importé.\n",
    ">\n",
    ">\n",
    "> * **Étape 2** : Instancier les couches qui composeront le modèle avec leur constructeur. Pour instancier une couche dense, il faut utiliser le constructeur `Dense`, que nous avons aussi importé.\n",
    ">\n",
    ">\n",
    "> * **Étape 3** : Ajouter les couches au modèle grâce à sa méthode `add`.\n",
    ">\n",
    ">\n",
    "> L'instanciation de couches contient plusieurs nuances:\n",
    ">\n",
    "> * La première couche que nous allons ajouter au modèle doit être instanciée **en précisant les dimensions du vecteur d'entrée** avec le paramètre **input_size**. Cette précision n'est pas nécessaire pour les couches suivantes.\n",
    ">\n",
    ">\n",
    "> * Le **nombre de neurones** dans une couche se définit avec le paramètre **units**.\n",
    ">\n",
    ">\n",
    "> * L'**initialisation des vecteurs de poids des neurones** se fait avec le paramètre **kernel_initializer**.\n",
    ">\n",
    ">\n",
    "> * Pour ajouter une fonction activation à une couche de neurones, on peut soit instancier une couche d'activation puis l'ajouter au modèle, ou bien définir la fonction d'activation dans le paramètre **activation** du constructeur d'une couche.\n",
    ">\n",
    ">\n",
    "> Plus d'infos sur les layers et leurs paramètres [ici](https://keras.io/layers/core/).\n",
    "> Plus d'infos sur les fonctions d'activations [ici](https://keras.io/activations/).\n",
    "\n",
    "* Instancier un modèle séquentiel qui sera appelé **model** grâce au constructeur `Sequential`.\n",
    "\n",
    "\n",
    "* Instancier une couche dense appelée **first_layer** avec 20 neurones telle que la dimension de son entrée soit le nombre de pixels de chaque image. Cette couche aura comme fonction d'activation la fonction `tanh`. Le vecteur de poids de cette couche seront initialisés aléatoirement selon la loi `normal`. \n",
    "\n",
    "\n",
    "* Instancier une deuxième couche dense appelée **second_layer** contenant autant de neurones que la base contient de classes, c'est-à-dire 10. Les poids de cette couche seront aussi intialisés aléatoirement selon une loi `normal` et la couche aura comme fonction d'activation `softmax`.\n",
    "\n",
    "\n",
    "* Ajouter au modèle les couches que nous avons instanciées. Il faudra les ajouter dans l'ordre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "function": "submission",
    "question_id": 3
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 4,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> Notre modèle est maintenant défini.\n",
    ">\n",
    "> Avant de pouvoir l'entraîner, il faut configurer son processus d'entraînement avec la méthode `compile`, qui reçoit **trois arguments**:\n",
    ">> * Un **optimiseur** (paramètre **optimizer**) qui définit l'algorithme d'optimisation que nous allons utiliser pour faire la descente de gradient de la fonction de perte.\n",
    ">>\n",
    ">>\n",
    ">> * Une **fonction de perte** (paramètre **loss**) à optimiser.\n",
    ">>\n",
    ">>\n",
    ">> * Une **liste de métriques** (paramètre **metrics**) utilisées pour évaluer la performance du modèle au fur et à mesure qu'il s'entraîne. La métrique *'accuracy'* est utilisée pour évaluer la précision de la classification.\n",
    "\n",
    "* Compiler le modèle grâce à sa méthode `compile`. La fonction de perte utilisée sera **'categorical_crossentropy'**, l'optimiseur sera **'adam'** et la métrique sera **['accuracy']**.\n",
    "\n",
    "Plus d'infos sur la compilation [ici](https://keras.io/getting-started/sequential-model-guide/#compilation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "function": "submission",
    "question_id": 4
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 5,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> Contrairement aux modèles classiques de Machine Learning, l'entraînement du modèle doit se faire ici par \"***batchs***\" :\n",
    "On divise la base de données d'entraînement en plusieurs *batchs* (ou lots) de la taille que l'on souhaite sur lesquels nous effectuons la descente de gradient. L'entraînement par *batchs* est nécessaire pour contourner les **problèmes de mémoire** des ordinateurs que nous utilisons de nos jours.\n",
    ">\n",
    "> Lorsque toutes les données ont été utilisées, on dit que l'entraînement a complété une \"***epoch***\". On peut choisir le nombre d'*epochs* sur lesquelles le modèle va s'entraîner.\n",
    ">\n",
    "> Par exemple, si nous choisissons de faire notre entraînement sur des *batchs* de taille 200 alors que la base de données contient 2000 entrées, chaque *epoch* consistera à faire la descente de gradient sur 2000/200 = 10 *batchs*.\n",
    ">\n",
    ">\n",
    "> Malheuresement, il n'y a pas de moyen analytique permettant de déterminer à l'avance le nombre d'*epochs*, l'*optimizer* ou la taille de batch qui donnera le meilleur résultat.\n",
    ">\n",
    "> Pour l'instant, la méthode la plus efficace pour choisir est d'entraîner le modèle plusieurs fois en faisant varier un de ces hyperparamètres à la fois et de choisir la combinaison de paramètres qui donne le meilleur résultat sur l'échantillon de validation.\n",
    "\n",
    "* Entraîner le modèle sur les données X_train et y_train grâce à la méthode `fit` du modèle:\n",
    "    * L'entraînement devra se faire sur 20 *epochs* (paramètre **epochs**)\n",
    "     \n",
    "    * Les *batchs* devront avoir une taille de 200 (paramètre **batch_size**)\n",
    "    \n",
    "    * La perfomance du modèle devra être évaluée sur un échantillon de validation contenant 20% des données (paramètre **validation_split**)\n",
    "    \n",
    "    * La sortie de l'entraînement devra être stockée dans une variable nommée **training_history**.\n",
    "\n",
    " \n",
    "Plus d'informations sur les paramètres de la méthode `fit` [ici](https://keras.io/models/sequential/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "function": "solution",
    "question_id": 5
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 7,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> La méthode `fit` renvoie en fait un objet de la classe **History**. Cet objet contient de nombreuses informations sur le déroulement de l'entraînement, en particulier les précisions sur les échantillons d'entraînement et de validation à la fin de chaque époque. Nous pouvons ainsi tracer une courbe représentant leur évolution tout au long de l'entraînement.\n",
    ">\n",
    "> L'objet **History** dispose d'un attribut **history** qui est un dictionnaire contenant dans ses clés les précisions obtenues par le modèle.\n",
    "\n",
    "* Lancer la cellule suivante pour stocker les précisions d'entraînement et de validation obtenues pendant l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_acc = training_history.history['accuracy']\n",
    "val_acc = training_history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 10,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Tracer l'évolution des précisions tout au long de l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 6,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "* Prédire grâce à la méthode `predict` du modèle la classification de l'échantillon **X_test** dans un tableau appelée **test_pred**.\n",
    "\n",
    "\n",
    "* Évaluer le modèle sur les données de test grâce à sa méthode `evaluate`. Cette méthode renvoie une liste dont le premier élément est la valeur de la fonction de perte et le second est la précision du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "function": "submission",
    "question_id": 6
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 11,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> Nous voulons maintenant effectuer un diagnostique de notre modèle. Pour cela, nous allons calculer une matrice de confusion sur l'échantillon de test.\n",
    ">\n",
    "> Néanmoins, les labels de **y_test** sont des **vecteurs binaires** à cause de la transformation *one-hot* que nous avons effectuée.\n",
    "> De plus, si nous essayons de prédire la classe de l'échantillon de test, la méthode **`predict`** du modèle renvoie un **vecteur de probabilités** où chaque élément est la probabilité d'appartenance à la classe correspondant à son indice.\n",
    ">\n",
    "> Pour utiliser la fonction `classification_report` du sous-module **metrics** de **scikit-learn**, il faut que le vecteur de la prédiction et le vecteur de la classe réelle soient composés d'entiers.\n",
    ">\n",
    "> Nous allons alors utiliser la méthode `argmax` d'un *array* **numpy** pour savoir à quelle classe correspondent les vecteurs binaires et les vecteurs de probabilites.\n",
    "\n",
    "* Prédire les classes de l'échantillon **X_test** à l'aide de la méthode `predict` du modèle. Stocker le résultat dans un tableau nommé **test_pred**.\n",
    "\n",
    "\n",
    "* Appliquer la méthode `argmax` sur les tableaux **test_pred** et **y_test** pour obtenir des vecteurs d'entiers correspondant aux classes prédites et réelles. Il faudra passer l'argument 'axis = 1' pour que l'argmax soit calculée sur les colonnes et non les lignes. Stocker les sorties des appels de la méthode `argmax` dans des tableaux nommés **test_pred_class** et **y_test_class**.\n",
    "\n",
    "\n",
    "* Afficher un compte-rendu évaluatif détaillé de la perfomance du modèle grâce à la fonction `classification_report` du sous-module **metrics** de **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "function": "solution",
    "question_id": 12
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 12,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> On observe facilement que les chiffres 0, 1 et 6 sont toujours les mieux classés, malgré un écart beaucoup moins important avec les autres chiffres qu'avec la méthode **random forest**.\n",
    ">\n",
    "> De plus, la précision a augmenté d'environ 3% par rapport à l'éxercice précedent. Ceci nous invite à considérer les modèles à réseaux de neurones, même simples, sont pour ce problème plus performants que les techniques de **random forest**.\n",
    ">\n",
    "> On s'intéresse maintenant aux erreurs réelles du modèle sur l'échantillon de test.\n",
    "\n",
    "* Calculer et afficher la matrice de confusion entre **y_test_class** et **test_pred_class**, appelée **cnf_matrix**, grâce à la fonction `confusion_matrix` du sous-module **metrics** de **scikit-learn**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "function": "submission",
    "question_id": 8
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "function": "preliminary",
    "question_id": 13,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "> On remarque que les chiffres 5, 3 et 8 sont souvent confondus. Ces chiffres semblent être les points faibles de notre modèle.\n",
    ">\n",
    "> Dans l'exercice suivant, nous essaierons d'améliorer notre modèle avec des couches de *convolution*."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Default Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "n_questions": 10
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
